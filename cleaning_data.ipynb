{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning City_name\n",
    "\n",
    "my target is categorize the city_name to fixed name for each city.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/fixed/data_science_dataset_wuzzuf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** there are  478 unique city name\n",
      "**  104  unique city names after filteration\n",
      "**  6  Arabic city names\n",
      "**  98  English city names\n"
     ]
    }
   ],
   "source": [
    "# cleaning The \"city_name\" column \n",
    "# my target is to add each city to same name.\n",
    "# print the unique city names entered.\n",
    "\n",
    "# get the unique cities, print them to console + output them to a file \n",
    "unique_cities = np.unique(data.city_name)\n",
    "print \"** there are \", len(np.unique(data.city_name)), \"unique city name\"\n",
    "with open('./data/generated/munging/unique_cities_unexpanded','wb') as f:\n",
    "    for item in np.unique(data.city_name):\n",
    "        f.write(item+\"\\n\")\n",
    "\n",
    "# Get each city with it's counts value, then extract the cities which has more than or equal three counts.\n",
    "value_counts = data.city_name.value_counts()\n",
    "unique_above_x = value_counts[value_counts>=3].keys()\n",
    "\n",
    "#for the extracted cities , lower/strip them\n",
    "unique_extracted = np.unique([ city.lower().strip() for city in unique_above_x ]) \n",
    "# then for the values which has more than city split them to a list, example \"cairo and alex\" > ['cairo','alex']\n",
    "unique_extracted = [ re.split('[.,/&\\-]|\\sand\\s|\\sor\\s',city,re.UNICODE) for city in unique_extracted ]\n",
    "\n",
    "# convert the list of lists to a single list with all unique city names, and save them to a file\n",
    "unique_list = [ s.strip() for item in unique_extracted for s in item  ]\n",
    "unique_list = np.unique(unique_list)\n",
    "with open('./data/generated/munging/unique_cities_expanded','wb') as f:\n",
    "    for item in unique_list:\n",
    "        f.write(item+'\\n')\n",
    "\n",
    "# add two more lists, one for english-typed city names, and the other for arabic-typed ones.\n",
    "# patter to detect english characters\n",
    "english_pattern = re.compile('^[\\d\\w\\s]*$')\n",
    "unique_english_list = []\n",
    "unique_arabic_list = []\n",
    "for city in unique_list:\n",
    "    if english_pattern.match(city) :\n",
    "        unique_english_list.append(city)\n",
    "    else:\n",
    "        unique_arabic_list.append(city)  \n",
    "\n",
    "# print some stuff :)\n",
    "print \"** \", len(unique_list), \" unique city names after filteration\"\n",
    "print \"** \", len(unique_arabic_list), \" Arabic city names\"\n",
    "print \"** \", len(unique_english_list), \" English city names\"        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this json file includes standard city names as a key, and all the possible names as values.\n",
    "with open('./data/fixed/cities_names_dict.json','rb') as f:\n",
    "    cities_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function takes a row, then change city name either to the standard name, or to 'other(\"name\")'\n",
    "# this done using the json file imported above.\n",
    "def clean_city_name(d):\n",
    "    city_name = d.city_name.lower().strip()\n",
    "    cities = re.split('[.,/&\\-]|\\sand\\s|\\sor\\s',city_name)\n",
    "    new_cities = []\n",
    "    for c in cities :\n",
    "        if c == \"\": break;\n",
    "        flag = True\n",
    "        for item in cities_dict :\n",
    "            for ii in cities_dict[item] :\n",
    "                if re.search(ii, c) :\n",
    "                    new_cities.append(item);\n",
    "                    flag = False;\n",
    "                    break\n",
    "        if flag == True :\n",
    "            new_cities.append(\"other('\"+c.lower().strip()+\"')\")\n",
    "    \n",
    "    cities_str = \"\"\n",
    "    for c in new_cities :\n",
    "        if cities_str == \"\" : \n",
    "            cities_str += c\n",
    "        else : \n",
    "            cities_str = cities_str +\", \"+c\n",
    "    d.city_name = cities_str\n",
    "    return d\n",
    "                \n",
    "new_data = data.apply(clean_city_name, axis=1)\n",
    "# 297 others, and 21,533 checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now i want to expand the rows who have more than city_name to separted rows,\n",
    "# example for city_name \"cairo,alex\" i want two rows on for cairo and one for alex.\n",
    "\n",
    "# First i'll make a new dataframe \"df2\" to hold the rows I want to separate\n",
    "#    and delete these rows from the main dataframe \"new_data\"\n",
    "df2 = pd.DataFrame(columns=new_data.columns)\n",
    "for x in new_data.iterrows() :\n",
    "    index = x[0]\n",
    "    row = x[1]\n",
    "    if re.search(\", \", row.city_name):\n",
    "        df2.loc[len(df2)] = row\n",
    "        new_data.drop(index, inplace=True)\n",
    "# second part for the \"df2\" dataframe i'll expand the rows inside it.  \n",
    "indexend = len(df2)\n",
    "for x in df2.iterrows():\n",
    "    index = x[0]\n",
    "    row = x[1]\n",
    "    cities = row.city_name.split(', ')\n",
    "    for city in cities :\n",
    "        alt = row\n",
    "        alt.city_name = city\n",
    "        df2.loc[indexend] = alt\n",
    "        indexend += 1 \n",
    "    df2.drop(index,inplace=True)\n",
    "# third part is to concat this dataframe to the main dataframe \"new_data\",\n",
    "# but before it lets reset the index for both of them\n",
    "df2.reset_index(drop=True, inplace=True)\n",
    "new_data.reset_index(drop=True, inplace=True)\n",
    "index = len(new_data)\n",
    "for x in df2.iterrows():\n",
    "    row = x[1]\n",
    "    new_data.loc[index] = row\n",
    "    index += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function takes a series [from our data] and format the date to %Y-%m ex: 2016-2\n",
    "def format_date(r):\n",
    "    r.post_date = re.findall(\"^\\d{4}-\\d{2}\", r.post_date)[0]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now I want to make a new dataframe to be the same as \"new_data\" but without the other(\"\") cities.\n",
    "data_cleaned = new_data.copy()\n",
    "for x in data_cleaned.iterrows():\n",
    "    index, row = x\n",
    "    if re.search('other', row.city_name):\n",
    "        data_cleaned.drop(index, inplace=True)\n",
    "# reset it's index, format the year and save to a file.\n",
    "data_cleaned.reset_index(drop=True, inplace=True)\n",
    "data_cleaned = data_cleaned.apply(format_date, axis=1)\n",
    "data_cleaned.to_csv('./data/generated/wazzuf/wuzzuf_data_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here I save it but without the two columns \"descrip, requirements\" as they hold huge data.\n",
    "data_cleaned_light = data_cleaned.drop(\n",
    "    [\"description\",'job_requirements'], axis=1) \n",
    "data_cleaned_light.to_csv('./data/generated/wazzuf/wuzzuf_data_cleaned_light.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in case you want the data for some city/ies just put them in the following list and run this cell.\n",
    "cities = [u'cairo', u'giza', u'alexandria', u'6th of october', u'10th of ramadan',\n",
    "       u'mansoura', u'any']\n",
    "\n",
    "data_with_some_cities = data_cleaned_light[ data_cleaned_light.city_name.isin(cities) ]\n",
    "\n",
    "data_with_some_cities.to_csv('./data/generated/wazzuf/wuzzuf_data_with_some_cities.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'format_date' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8c37cd543b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_cleaned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata_cleaned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/generated/wazzuf/wuzzuf_data_formatedate.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'format_date' is not defined"
     ]
    }
   ],
   "source": [
    "# save the cleaned data with date formated.\n",
    "data_cleaned = data.apply(format_date, axis=1)\n",
    "data_cleaned.to_csv('./data/generated/wazzuf/wuzzuf_data_formateddate.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
